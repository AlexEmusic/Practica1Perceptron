# -*- coding: utf-8 -*-
"""Gradiente.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13jS4EpU8xUbNLISD87LvM0SSWttMIehX
"""

import numpy as np

def calcular_gradiente(funcion, punto, epsilon=1e-5):
    """
    Calcula el gradiente de una función en un punto dado utilizando el método de diferencias finitas.

    :param funcion: La función a la que se calculará el gradiente.
    :param punto: El punto en el que se calculará el gradiente.
    :param epsilon: Tamaño del paso para el cálculo de diferencias finitas.
    :return: El gradiente de la función en el punto dado.
    """
    dimension = len(punto)  # Obtén la dimensión del punto de partida.
    gradiente = np.zeros(dimension)  # Inicializa un vector gradiente de ceros.

    # Recorre cada dimensión del punto.
    for i in range(dimension):
        perturbacion = np.zeros(dimension)
        perturbacion[i] = epsilon  # Crea una perturbación solo en la dimensión actual.

        # Calcula la diferencia finita para la dimensión actual y almacena el resultado en el gradiente.
        gradiente[i] = (funcion(punto + perturbacion) - funcion(punto - perturbacion)) / (2 * epsilon)

    return gradiente

def funcion_a_optimizar(x):
    x1, x2 = x[0], x[1]
    # Define aquí la función que deseas optimizar.
    # En este caso, la función es f(x1, x2) = 10 - e^(-((x1^2) + 3*(x2^2))).
    return 10 - np.exp(-((x1**2) + 3*(x2**2)))

# Configuración de hiperparámetros
learning_rate = 0.1  # Tasa de aprendizaje: controla el tamaño de los pasos en la optimización.
num_iteraciones = 100  # Número de iteraciones: cuántas veces se ajustan los pesos.

# Punto de inicio para la optimización
punto_inicial = np.array([1.0, 1.0])  # El punto desde el cual comenzamos a buscar el mínimo.

# Bucle de optimización
for iteracion in range(num_iteraciones):
    gradiente = calcular_gradiente(funcion_a_optimizar, punto_inicial)
    punto_inicial -= learning_rate * gradiente  # Actualiza el punto de inicio en la dirección opuesta al gradiente.

# Imprime los resultados
print("Valor mínimo encontrado en:", punto_inicial)
print("Valor de la función en el mínimo:", funcion_a_optimizar(punto_inicial))